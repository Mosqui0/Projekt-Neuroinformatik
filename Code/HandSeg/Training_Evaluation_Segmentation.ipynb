{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.9\n"
     ]
    }
   ],
   "source": [
    "# Library header\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import Model, Input\n",
    "from tqdm import tqdm\n",
    "import shutil #library for moving files\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# own libraries\n",
    "import split_dataset as spData\n",
    "import handseg_two_classes as hand\n",
    "import data_gen as dg\n",
    "\n",
    "\n",
    "#globals\n",
    "input_width = 640\n",
    "input_height = 480\n",
    "handseg_path = '../../../handseg-150k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(img):\n",
    "    return (img / img.max()) * 255\n",
    "\n",
    "# create the data generators with data_loader\n",
    "# ... for training\n",
    "train_gen = dg.image_segmentation_generator(\n",
    "        images_path=handseg_path+\"/images/\",\n",
    "        segs_path=handseg_path+\"/masks/\", \n",
    "        batch_size=32,\n",
    "        n_classes=2,\n",
    "        input_height=480,\n",
    "        input_width=640,\n",
    "        output_height=480,\n",
    "        output_width=640,\n",
    "        do_augment=False,\n",
    "        preprocessing=preprocess,\n",
    "        read_image_type=1)\n",
    "# read_image_type -> 0 = grayscale; -> 1 = rgb\n",
    "\n",
    "# ... for validation\n",
    "val_gen = dg.image_segmentation_generator(\n",
    "        images_path=handseg_path+\"/val_images/\",\n",
    "        segs_path=handseg_path+\"/val_masks/\",\n",
    "        batch_size=32,\n",
    "        n_classes=2,\n",
    "        input_height=480,\n",
    "        input_width=640,\n",
    "        output_height=480,\n",
    "        output_width=640,\n",
    "        preprocessing=preprocess,\n",
    "        read_image_type=1)\n",
    "\n",
    "# ... for testing\n",
    "test_gen = dg.image_segmentation_generator(\n",
    "        images_path=handseg_path+\"/test_images/\",\n",
    "        segs_path=handseg_path+\"/test_masks/\",\n",
    "        batch_size=2,\n",
    "        n_classes=2,\n",
    "        input_height=480,\n",
    "        input_width=640,\n",
    "        output_height=480,\n",
    "        output_width=640,\n",
    "        preprocessing=preprocess,\n",
    "        read_image_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is a (480, 640, 2) np.array -> interpret the softmax values in third dimension\n",
    "def interpretPrediction(pred):\n",
    "  # create np.array to save the interpretation\n",
    "  interpretation = np.zeros((480, 640))\n",
    "  height, width, _ = pred.shape\n",
    "  for i in range(height):\n",
    "    for j in range(width):\n",
    "      # create a list of the probabilites from each class\n",
    "      value_list = [pred[i,j,0], pred[i,j,1]]\n",
    "      # find out, which class has been predicted\n",
    "      largest = max(value_list)\n",
    "      index_of_largest = value_list.index(largest)\n",
    "      # store new value in the interpretation array\n",
    "      interpretation[i,j] = index_of_largest\n",
    "  return interpretation\n",
    "\n",
    "def drawBoxes(mask, img):\n",
    "  # Iterate all colors in mask\n",
    "  for color in np.unique(mask):\n",
    "\n",
    "      # Color 0 is assumed to be background or artifacts\n",
    "      if color == 0:\n",
    "          continue\n",
    "\n",
    "      # Determine bounding rectangle w.r.t. all pixels of the mask with\n",
    "      # the current color\n",
    "      x, y, w, h = cv2.boundingRect(np.uint8(mask == color))\n",
    "      print(x,y,w,h)\n",
    "      # Draw bounding rectangle to color image\n",
    "      out = cv2.rectangle(img.copy(), (x, y), (x+w, y+h), (255, int(color), 0), 2)\n",
    "\n",
    "      # Show image with bounding box\n",
    "      plt.imshow(out, cmap=\"Greys\"); plt.title('img_' + str(color)); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from previous training\n",
    "handseg.load_weights(\"./handseg_model_twoClasses_3.h5\")\n",
    "\n",
    "# training\n",
    "history = handseg.fit(x=train_gen,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            steps_per_epoch=512,\n",
    "            validation_data=val_gen,\n",
    "            validation_batch_size=32,\n",
    "            validation_steps=256)\n",
    "\n",
    "# load/save the already trained weigths\n",
    "handseg.save_weights(\"./handseg_model_twoClasses_updated.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# returns a list of tupels containing the values for iou and accuracy\n",
    "# since the metrics are updated in each step, the las tupel value containst the\n",
    "# final result\n",
    "# load the weights from previous training\n",
    "handseg.load_weights(\"./handseg_model_twoClasses_1.h5\")\n",
    "\n",
    "def eval_handseg():\n",
    "  # path values for test data\n",
    "  paths_test_images = glob.glob(handseg_path+\"/test_images/*.png\")\n",
    "  paths_test_mask = glob.glob(handseg_path+\"/test_masks/*.png\")\n",
    "\n",
    "  # create the respective metrics\n",
    "  metric_iou = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "  metric_acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "  # initialize list\n",
    "  result = []\n",
    "\n",
    "  for i in tqdm(range(len(paths_test_images))):\n",
    "    # load the img\n",
    "    in_image = cv2.imread(paths_test_images[i])\n",
    "    in_image = np.reshape(in_image, (1,480, 640, 3))\n",
    "    # apply preprocessing\n",
    "    in_image = preprocess(in_image)\n",
    "    # predict\n",
    "    out_im = handseg.predict(in_image)\n",
    "    # interpret prediction\n",
    "    out_im = interpretPrediction(out_im[0])\n",
    "    # calculate IoU and accuracy\n",
    "    gt = cv2.imread(paths_test_mask[i])\n",
    "    metric_iou.update_state(gt[:,:,0], out_im)\n",
    "    metric_acc.update_state(gt[:,:,0], out_im)\n",
    "    # append tupel to list\n",
    "    result.append((metric_iou.result().numpy(), metric_acc.result().numpy()))\n",
    "\n",
    "  return result\n",
    "\n",
    "res = eval_handseg()\n",
    "# print results -> last element in list show the result\n",
    "print(res[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
