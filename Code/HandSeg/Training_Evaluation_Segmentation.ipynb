{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.9\n"
     ]
    }
   ],
   "source": [
    "# Library header\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import Model, Input\n",
    "from tqdm import tqdm\n",
    "import shutil #library for moving files\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# own libraries\n",
    "import split_dataset as spData\n",
    "import handseg_two_classes as hand\n",
    "import data_gen as dg\n",
    "import util as ut\n",
    "\n",
    "#globals\n",
    "input_width = 640\n",
    "input_height = 480\n",
    "handseg_path = '../../../handseg-150k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(img):\n",
    "    return (img / img.max()) * 255\n",
    "\n",
    "# create the data generators with data_loader\n",
    "# ... for training\n",
    "train_gen = dg.image_segmentation_generator(\n",
    "        images_path=handseg_path+\"/images/\",\n",
    "        segs_path=handseg_path+\"/masks/\", \n",
    "        batch_size=32,\n",
    "        n_classes=2,\n",
    "        input_height=480,\n",
    "        input_width=640,\n",
    "        output_height=480,\n",
    "        output_width=640,\n",
    "        do_augment=False,\n",
    "        preprocessing=preprocess,\n",
    "        read_image_type=1)\n",
    "# read_image_type -> 0 = grayscale; -> 1 = rgb\n",
    "\n",
    "# ... for validation\n",
    "val_gen = dg.image_segmentation_generator(\n",
    "        images_path=handseg_path+\"/val_images/\",\n",
    "        segs_path=handseg_path+\"/val_masks/\",\n",
    "        batch_size=32,\n",
    "        n_classes=2,\n",
    "        input_height=480,\n",
    "        input_width=640,\n",
    "        output_height=480,\n",
    "        output_width=640,\n",
    "        preprocessing=preprocess,\n",
    "        read_image_type=1)\n",
    "\n",
    "# ... for testing\n",
    "test_gen = dg.image_segmentation_generator(\n",
    "        images_path=handseg_path+\"/test_images/\",\n",
    "        segs_path=handseg_path+\"/test_masks/\",\n",
    "        batch_size=2,\n",
    "        n_classes=2,\n",
    "        input_height=480,\n",
    "        input_width=640,\n",
    "        output_height=480,\n",
    "        output_width=640,\n",
    "        preprocessing=preprocess,\n",
    "        read_image_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from previous training\n",
    "handseg.load_weights(\"./handseg_model_twoClasses_4.h5\")\n",
    "\n",
    "# training\n",
    "history = handseg.fit(x=train_gen,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            steps_per_epoch=512,\n",
    "            validation_data=val_gen,\n",
    "            validation_batch_size=16,\n",
    "            validation_steps=256)\n",
    "\n",
    "# load/save the already trained weigths\n",
    "handseg.save_weights(\"./handseg_model_twoClasses_updated.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# returns a list of tupels containing the values for iou and accuracy\n",
    "# since the metrics are updated in each step, the las tupel value containst the\n",
    "# final result\n",
    "# load the weights from previous training\n",
    "handseg.load_weights(\"../../../results/weights/twoClasses/handseg_model_twoClasses_4.h5\")\n",
    "\n",
    "def eval_handseg():\n",
    "  # path values for test data\n",
    "  paths_test_images = glob.glob(handseg_path+\"/test_images/*.png\")\n",
    "  paths_test_mask = glob.glob(handseg_path+\"/test_masks/*.png\")\n",
    "\n",
    "  # create the respective metrics\n",
    "  metric_iou = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "  metric_acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "  # initialize list\n",
    "  result = []\n",
    "\n",
    "  for i in tqdm(range(len(paths_test_images))):\n",
    "    # load img\n",
    "    in_image = cv2.imread(paths_test_images[i])\n",
    "    in_image = np.reshape(in_image, (1,480, 640, 3))\n",
    "    # apply preprocessing\n",
    "    in_image = ut.preprocess(in_image)\n",
    "    # predict\n",
    "    out_im = handseg.predict(in_image)\n",
    "    # interpret prediction\n",
    "    out_im = ut.interpretPrediction(out_im[0])\n",
    "    # load mask image\n",
    "    gt = cv2.imread(paths_test_mask[i])\n",
    "    \n",
    "    # apply preprocessing to mask images (3 -> 2 classes)\n",
    "    gt = tf.where(gt == 2, 1, gt)\n",
    "    \n",
    "    # calculate IoU and accuracy\n",
    "    metric_iou.update_state(gt[:,:,0], out_im)\n",
    "    metric_acc.update_state(gt[:,:,0], out_im)\n",
    "    # append tupel to list\n",
    "    result.append((metric_iou.result().numpy(), metric_acc.result().numpy()))\n",
    "\n",
    "    # reset the metric states\n",
    "    metric_iou.reset_states()\n",
    "    metric_acc.reset_states()\n",
    "\n",
    "  return result\n",
    "\n",
    "res = eval_handseg()\n",
    "n = len(res)\n",
    "sum_iou = 0\n",
    "sum_acc = 0\n",
    "for values in res:\n",
    "    sum_iou += values[0]\n",
    "    sum_acc += values[1]\n",
    "mean_iou = sum_iou/n\n",
    "mean_acc = sum_acc/n\n",
    "print(\"Mean IoU: \", mean_iou)\n",
    "print(\"Mean Acc: \", mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are helper functions for evaluation\n",
    "# this function will create a segmentation and create the bounding box\n",
    "def predictBox(img, id):\n",
    "    #reshape input image\n",
    "    input_image = cv2.resize(img, (640, 480))\n",
    "    input_image = np.reshape(input_image, (1,480, 640, 3))\n",
    "    # apply preprocessing\n",
    "    input_image = preprocess2(input_image)\n",
    "    # predict hand segmentation\n",
    "    output_image = handseg.predict(input_image)\n",
    "    # interpret prediction\n",
    "    output_image = ut.interpretPrediction(output_image[0])\n",
    "    # draw the bounding box\n",
    "    box = drawBoxes_eval(output_image, img, id)\n",
    "    return box\n",
    "\n",
    "# get the bounding box values and save the image inside\n",
    "def drawBoxes_eval(mask, img, id):\n",
    "    # Iterate all colors in mask\n",
    "    for color in np.unique(mask):\n",
    "\n",
    "        # Color 0 is assumed to be background or artifacts\n",
    "        if color == 0:\n",
    "          continue\n",
    "\n",
    "        # Determine bounding rectangle w.r.t. all pixels of the mask with\n",
    "        # the current color\n",
    "        x, y, w, h = cv2.boundingRect(np.uint8(mask == color))\n",
    "        # Draw bounding rectangle to color image\n",
    "        out = cv2.rectangle(img.copy(), (x, y), (x+w, y+h), (0, int(color), 255), 2)\n",
    "\n",
    "        # Show image with bounding box\n",
    "        #plt.imshow(out); plt.title('img_' + str(color)); plt.show()\n",
    "        cv2.imwrite(\"../../../BoundingBox_eval/results/\"+id+\".png\", out)\n",
    "    return (x,y,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create eval for bounding box samples\n",
    "# load weights\n",
    "handseg.load_weights(\"../../../results/weights/twoClasses/handseg_model_twoClasses_4.h5\")\n",
    "\n",
    "# create eval directory\n",
    "os.mkdir(\"../../../BoundingBox_eval\")\n",
    "# ... and respective subdirectories\n",
    "os.mkdir(\"../../../BoundingBox_eval/test_seq_1\")\n",
    "os.mkdir(\"../../../BoundingBox_eval/test_seq_2\")\n",
    "os.mkdir(\"../../../BoundingBox_eval/results\")\n",
    "\n",
    "# initialize paths\n",
    "sample_path_1 = \"../../../ICVL/Depth/test_seq_1\"\n",
    "sample_path_2 = \"../../../ICVL/Depth/test_seq_2\"\n",
    "eval_path_1 = \"../../../BoundingBox_eval/test_seq_1\"\n",
    "eval_path_2 = \"../../../BoundingBox_eval/test_seq_2\"\n",
    "\n",
    "#load all file names into a list\n",
    "sample_paths_list_1 = glob.glob(sample_path_1+\"/*.png\")\n",
    "sample_paths_list_2 = glob.glob(sample_path_2+\"/*.png\")\n",
    "\n",
    "# shuffle paths and take first 200 samples\n",
    "random.shuffle(sample_paths_list_1)\n",
    "random.shuffle(sample_paths_list_2)\n",
    "\n",
    "# move samples to the eval path\n",
    "for i in range(100): \n",
    "    shutil.copy(sample_paths_list_1[i], eval_path_1)\n",
    "    shutil.copy(sample_paths_list_2[i], eval_path_2)\n",
    "\n",
    "\n",
    "# initialize list for evaluation samples\n",
    "eval_paths_list_1 = glob.glob(eval_path_1+\"/*.png\")\n",
    "eval_paths_list_2 = glob.glob(eval_path_2+\"/*.png\")\n",
    "\n",
    "# collect all samples in eval_path result\n",
    "for i in tqdm(range(100)):\n",
    "    image_name_1 = eval_paths_list_1[i]\n",
    "    img_1 = cv2.imread(image_name_1)\n",
    "    image_name_2 = eval_paths_list_2[i]\n",
    "    img_2 = cv2.imread(image_name_2)\n",
    "    # predict bounding box via hand segmentation\n",
    "    predictBox(img_1, str(i)+\"_1\")\n",
    "    predictBox(img_2, str(i)+\"_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
